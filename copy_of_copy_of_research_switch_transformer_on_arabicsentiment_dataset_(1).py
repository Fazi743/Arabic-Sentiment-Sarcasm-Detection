# -*- coding: utf-8 -*-
"""Copy_of_Copy_of_Research_Switch_Transformer_On_ArabicSentiment_Dataset_(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t10NkZrhTl-p_xMx3IIJpq4AxZzX9kUr
"""

!pip install -U tensorflow-addons

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow import keras

from tensorflow.keras import layers
import pandas as pd

!pip install datasets

from datasets import load_dataset
dataset = load_dataset("ar_sarcasm")

text = dataset['train']['tweet']

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import string
import re
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix,accuracy_score, classification_report

DATA = pd.DataFrame()
DATA['Tweets']=pd.Series(text)

from textblob import TextBlob

DATA['polarity'] = DATA['Tweets'].map(lambda text: TextBlob(text).sentiment.polarity)

DATA['subjectivity'] = DATA['Tweets'].map(lambda text: TextBlob(text).sentiment.subjectivity)

dataset['train']

DATA=pd.DataFrame()
DATA['tweets'] = pd.Series(dataset['train']['tweet'])

DATA['dialect'] = pd.Series(dataset['train']['dialect'])

DATA['sarcasm'] = pd.Series(dataset['train']['sarcasm'])

DATA['sentiment'] = pd.Series(dataset['train']['sentiment'])

DATA.head()

DATA['dialect'].value_counts()

import seaborn as sns
sns.countplot(x=DATA['dialect'])

sns.countplot(x=DATA['sarcasm'])

sns.countplot(x=DATA['sentiment'])

import numpy as np
num_tokens_per_example=200

from keras.preprocessing.text import text_to_word_sequence
# tokenize the document
def GET_DATA(DATA, LABELS):
  result = np.array([text_to_word_sequence(i) for i in pd.Series(DATA)])
  VOCAB=[]
  for list_ in pd.Series(result).to_list():
    for i in list_:
      VOCAB.append(i)
  # vocabolary = ','.join(VOCAB)
  from keras.preprocessing.text import one_hot
  vocab_size = len(set(VOCAB))
  vocab_size=20000
  Result = np.array([one_hot(' '.join(i), round(vocab_size)) for i in result])
  return keras.preprocessing.sequence.pad_sequences( Result, maxlen=num_tokens_per_example, padding='post', truncating='post'), LABELS, vocab_size

dataset.data['train']

X1,Y1, vocab_size=GET_DATA(pd.Series(np.array(dataset.data['train']['tweet'])), pd.Series(np.array(dataset.data['train']['sentiment'])))
X2,Y2, _ =GET_DATA(pd.Series(np.array(dataset.data['test']['tweet'])), pd.Series(np.array(dataset.data['test']['sentiment'])))
X2.shape, Y2.shape

from sklearn.feature_extraction.text import CountVectorizer
x_train = CountVectorizer(max_features=40).fit_transform(np.array(dataset.data['train']['tweet'])).toarray()[:8400,:]
x_test = CountVectorizer(max_features=40).fit_transform(np.array(dataset.data['test']['tweet'])).toarray()[:2100,:]
y_train = np.array(dataset.data['train']['sentiment'])[:8400]
y_test = np.array(dataset.data['test']['sentiment'])[:2100]
# num_tokens_per_example=40
x_train.shape, y_train.shape, x_test.shape, y_test.shape

vocab_size=40

pd.Series(y_train).unique()

x_train,y_train = X1[:8400], Y1[:8400]
x_test,y_test = X2[:2100], Y2[:2100]
x_train.shape,y_train.shape

y_train.value_counts()

from keras.utils import to_categorical
y_train= to_categorical(y_train,3)
y_test= to_categorical(y_test,3)
# y_test= to_categorical(y_test,2)
y_train

pd.Series(np.argmax(y_train,axis=1)).head()

embed_dim = 128  # Embedding size for each token.
num_heads = 2  # Number of attention heads
ff_dim = 128  # Hidden layer size in feedforward network.
num_experts = 10  # Number of experts used in the Switch Transformer.
batch_size = 50  # Batch size.
learning_rate = 0.001  # Learning rate.
dropout_rate = 0.2  # Dropout rate.
num_epochs = 3  # Number of epochs.
num_tokens_per_batch = (
    batch_size * num_tokens_per_example
)  # Total number of tokens per batch.
print(f"Number of tokens per batch: {num_tokens_per_batch}")

embedding_dim = 128

import tensorflow_probability as tfp

# kernel_size: the number of parameters in the dense weight matrix
def prior(kernel_size, bias_size, dtype=None):
    n = kernel_size + bias_size
    # Independent Normal Distribution
    return lambda t: tfp.distributions.Independent(tfp.distributions.Normal(loc=tf.zeros(n, dtype=dtype),
                                                scale=1),
                                     reinterpreted_batch_ndims=1)

def posterior(kernel_size, bias_size, dtype=None):
    n = kernel_size + bias_size
    return keras.models.Sequential([
        tfp.layers.VariableLayer(tfp.layers.IndependentNormal.params_size(n), dtype=dtype),
        tfp.layers.IndependentNormal(n)
    ])

class gMLPLayer(layers.Layer):
    def __init__(self, embedding_dim, *args, **kwargs):
        super(gMLPLayer, self).__init__(*args, **kwargs)
        self.channel_projection1 = tfp.layers.DenseVariational(embedding_dim * 2, posterior , prior, activation='relu')
        self.channel_projection2 = tfp.layers.DenseVariational(embedding_dim, posterior , prior)
        self.spatial_projection = tfp.layers.DenseVariational(200, posterior , prior )
        # self.channel_projection1 = layers.Dense(embedding_dim * 2, activation='relu')
        # self.channel_projection2 = layers.Dense(embedding_dim)
        # self.spatial_projection = layers.Dense(200)
        self.normalize1 = layers.LayerNormalization(epsilon=1e-6)
        self.normalize2 = layers.LayerNormalization(epsilon=1e-6)
        self.Inp=layers.Dense(units=embedding_dim)
    def spatial_gating_unit(self, x):
        # Split x along the channel dimensions.
        # Tensors u and v will in th shape of [batch_size, num_patchs, embedding_dim].
        u, v = tf.split(x, num_or_size_splits=2, axis=2)
        v = self.normalize2(v)
        # Apply spatial projection.
        v_channels = tf.linalg.matrix_transpose(v)
        v_projected = self.spatial_projection(v_channels)
        v_projected = tf.linalg.matrix_transpose(v_projected)
        # Apply element-wise multiplication.

        return u * v_projected
    def call(self, inputs):
        x = self.Inp(inputs)
        x = self.normalize1(x)
        # Apply the first channel projection. x_projected shape
        x_projected = self.channel_projection1(x)
        # Apply the spatial gating unit
        x_spatial = self.spatial_gating_unit(x_projected)
        # Apply the second channel projection
        x_projected = self.channel_projection2(x_spatial)
        return x + x_projected

class TokenAndPositionEmbedding(layers.Layer):
    def __init__(self, maxlen, vocab_size, embed_dim):
        super().__init__()
        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)
        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)
        self.Sapatial_Gated_Embedding=gMLPLayer(embedding_dim)
    def call(self, x):
        maxlen = tf.shape(x)[-1]
        positions = tf.range(start=0, limit=maxlen, delta=1)
        positions = self.pos_emb(positions)
        x = self.token_emb(x)
        return x + positions + self.Sapatial_Gated_Embedding(x)

def create_feedforward_network(ff_dim, name=None):
    Inp_=keras.layers.Input(shape=[128])
    D1= tfp.layers.DenseVariational(ff_dim*10, posterior, prior)(Inp_)
    D2= tfp.layers.DenseVariational(ff_dim*9, posterior, prior)(D1)
    D3= tfp.layers.DenseVariational(ff_dim*8, posterior, prior)(D2)
    D4= tfp.layers.DenseVariational(ff_dim*7, posterior, prior)(D3)
    D5= tfp.layers.DenseVariational(ff_dim*6, posterior, prior)(D4)
    concat1=keras.layers.concatenate([D1, D2, D3])
    concat2=keras.layers.concatenate([D2, D3, D4])
    concat3=keras.layers.concatenate([D3, D4, D5])
    D6= tfp.layers.DenseVariational(ff_dim*5, posterior, prior)(concat1)
    D7= tfp.layers.DenseVariational(ff_dim*4, posterior, prior)(concat2)
    D8= tfp.layers.DenseVariational(ff_dim*3, posterior, prior)(concat3)
    concat=keras.layers.concatenate([D6, D7, D8])
    out= tfp.layers.DenseVariational(ff_dim, posterior, prior)(concat)
    return keras.models.Model([Inp_],[out])


# def create_feedforward_network(ff_dim, name=None):
#   return keras.models.Sequential([tfp.layers.DenseVariational(ff_dim*3, posterior, prior),
#                                   tfp.layers.DenseVariational(ff_dim*2, posterior, prior),
#                                   tfp.layers.DenseVariational(ff_dim, posterior, prior)])

def load_balanced_loss(router_probs, expert_mask):
    num_experts = tf.shape(expert_mask)[-1]
    density = tf.reduce_mean(expert_mask, axis=0)
    density_proxy = tf.reduce_mean(router_probs, axis=0)
    loss = tf.reduce_mean(density_proxy * density) * tf.cast((num_experts ** 2), tf.dtypes.float32)
    return loss

class Router(layers.Layer):
    def __init__(self, num_experts, expert_capacity):
        self.num_experts = num_experts
        self.route = tfp.layers.DenseVariational(num_experts, posterior, prior)
        # self.route = keras.layers.Dense(num_experts)
        self.expert_capacity = expert_capacity
        super().__init__()
    def call(self, inputs, training=False):
        router_logits = self.route(inputs)
        if training:
           router_logits += tf.random.uniform(shape=router_logits.shape, minval=0.9, maxval=1.1)
        router_probs = keras.activations.softmax(router_logits, axis=-1)
        expert_gate, expert_index = tf.math.top_k(router_probs, k=1)
        expert_mask = tf.one_hot(expert_index, depth=self.num_experts)
        aux_loss = load_balanced_loss(router_probs, expert_mask)
        self.add_loss(aux_loss)
        position_in_expert = tf.cast(tf.math.cumsum(expert_mask, axis=0) * expert_mask, tf.dtypes.int32)
        expert_mask *= tf.cast(tf.math.less(tf.cast(position_in_expert, tf.dtypes.int32), self.expert_capacity),tf.dtypes.float32)
        expert_mask_flat = tf.reduce_sum(expert_mask, axis=-1)
        expert_gate *= expert_mask_flat
        combined_tensor = tf.expand_dims(
            expert_gate
            * expert_mask_flat
            * tf.squeeze(tf.one_hot(expert_index, depth=self.num_experts), 1),
            -1,
        ) * tf.squeeze(tf.one_hot(position_in_expert, depth=self.expert_capacity), 1)
        dispatch_tensor = tf.cast(combined_tensor, tf.dtypes.float32)
        return dispatch_tensor, combined_tensor

class Switch(layers.Layer):
    def __init__(self, num_experts, embed_dim, num_tokens_per_batch, capacity_factor=1):
        self.num_experts = num_experts
        self.embed_dim = embed_dim
        self.experts = [
            create_feedforward_network(embed_dim) for _ in range(num_experts)
        ]

        self.expert_capacity = num_tokens_per_batch // self.num_experts
        self.router = Router(self.num_experts, self.expert_capacity)
        super().__init__()

    def call(self, inputs):
        batch_size = tf.shape(inputs)[0]
        num_tokens_per_example = tf.shape(inputs)[1]

        # inputs shape: [num_tokens_per_batch, embed_dim]
        inputs = tf.reshape(inputs, [num_tokens_per_batch, self.embed_dim])
        # dispatch_tensor shape: [expert_capacity, num_experts, tokens_per_batch]
        # combine_tensor shape: [tokens_per_batch, num_experts, expert_capacity]
        dispatch_tensor, combine_tensor = self.router(inputs)
        # expert_inputs shape: [num_experts, expert_capacity, embed_dim]
        expert_inputs = tf.einsum("ab,acd->cdb", inputs, dispatch_tensor)
        expert_inputs = tf.reshape(
            expert_inputs, [self.num_experts, self.expert_capacity, self.embed_dim]
        )
        # Dispatch to experts
        expert_input_list = tf.unstack(expert_inputs, axis=0)
        print(expert_input_list)
        expert_output_list = [
            self.experts[idx](expert_input)
            for idx, expert_input in enumerate(expert_input_list)
        ]
        # expert_outputs shape: [expert_capacity, num_experts, embed_dim]
        expert_outputs = tf.stack(expert_output_list, axis=1)
        # expert_outputs_combined shape: [tokens_per_batch, embed_dim]
        expert_outputs_combined = tf.einsum(
            "abc,xba->xc", expert_outputs, combine_tensor
        )
        # output shape: [batch_size, num_tokens_per_example, embed_dim]
        outputs = tf.reshape(
            expert_outputs_combined,
            [batch_size, num_tokens_per_example, self.embed_dim],
        )
        return outputs

class TransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, ffn, dropout_rate=0.1):
        super().__init__()
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = ffn
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(dropout_rate)
        self.dropout2 = layers.Dropout(dropout_rate)
    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

def create_classifier():
    switch = Switch(num_experts, embed_dim, num_tokens_per_batch)
    transformer_block = TransformerBlock(ff_dim, num_heads, switch)

    inputs = layers.Input(shape=(num_tokens_per_example,))
    embedding_layer = TokenAndPositionEmbedding(
        num_tokens_per_example, vocab_size, embed_dim
    )
    x = embedding_layer(inputs)
    x = transformer_block(x)
    x = layers.GlobalAveragePooling1D()(x)
    x=keras.layers.Dropout(0.2)(x)
    x = layers.Dense(128, activation="relu")(x)
    x=keras.layers.Dropout(0.2)(x)
    x = layers.Dense(64, activation="relu")(x)
    outputs = layers.Dense(3, activation="sigmoid")(x)

    classifier = keras.Model(inputs=inputs, outputs=outputs)
    return classifier

import keras.backend as K
def F1_Score(y_true, y_pred): #taken from old keras source code
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    recall = true_positives / (possible_positives + K.epsilon())
    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())
    return f1_val

def run_experiment(classifier):
    classifier.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.0001),
        loss="categorical_crossentropy",
        metrics=[keras.metrics.CategoricalAccuracy(),keras.metrics.Recall(), keras.metrics.Precision(),F1_Score, tf.keras.metrics.AUC()],
    )
    history = classifier.fit(
        x_train,y_train,
        batch_size=50,
        epochs=20,
        validation_data=(x_test,y_test)
    )
    return history

classifier=create_classifier()
hist=run_experiment(classifier)

import matplotlib.pyplot as plt
plt.rcParams['figure.figsize']=(15,6)
pd.DataFrame(hist.history).plot()
plt.grid()
plt.gca().set_ylim(0,1)
plt.show()

'''
AOA, Sir me is study ko extend kar raha hon , hum ne sirf sarcasm detect kiya hy abi tak, but me abhi same dataset pe sentiment classification and Dialect detection bi kar raha hon yani 3 kam ho jae gai ?
'''

from sklearn.preprocessing import StandardScaler
scale = StandardScaler()
x_train_=scale.fit_transform(x_train)
x_test_=scale.fit_transform(x_test)
x_test_

import keras.backend as K
def F1_Score(y_true, y_pred): #taken from old keras source code
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    recall = true_positives / (possible_positives + K.epsilon())
    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())
    return f1_val

def run_experiment(classifier):
    classifier.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.0001),
        loss="categorical_crossentropy",
        metrics=[keras.metrics.CategoricalAccuracy(),keras.metrics.Recall(), keras.metrics.Precision(),F1_Score, tf.keras.metrics.AUC()],
    )
    history = classifier.fit(
        x_train_,y_train,
        batch_size=50,
        epochs=20,
        validation_data=(x_test_,y_test)
    )
    return history

classifier=create_classifier()
hist=run_experiment(classifier)

import matplotlib.pyplot as plt
plt.rcParams['figure.figsize']=(15,6)
pd.DataFrame(hist.history).plot()
plt.grid()
plt.gca().set_ylim(0,1)
plt.show()

"""## Random Forest"""

from sklearn.ensemble import RandomForestClassifier
model= RandomForestClassifier()
model.fit(x_train, np.argmax(y_train,axis=1))
model.score(x_test, np.argmax(y_test,axis=1))

from sklearn.metrics import classification_report
print(classification_report(np.argmax(y_test,axis=1), model.predict(x_test)))

from sklearn.svm import SVC
model= SVC()
model.fit(x_train, np.argmax(y_train,axis=1))
model.score(x_test, np.argmax(y_test,axis=1))

from sklearn.metrics import classification_report
print(classification_report(np.argmax(y_test,axis=1), model.predict(x_test)))

from sklearn.naive_bayes import MultinomialNB
model= MultinomialNB()
model.fit(x_train, np.argmax(y_train,axis=1))
model.score(x_test, np.argmax(y_test,axis=1))

from sklearn.metrics import classification_report
print(classification_report(np.argmax(y_test,axis=1), model.predict(x_test)))

from sklearn.neighbors import KNeighborsClassifier
model= KNeighborsClassifier()
model.fit(x_train, np.argmax(y_train,axis=1))
model.score(x_test, np.argmax(y_test,axis=1))

from sklearn.metrics import classification_report
print(classification_report(np.argmax(y_test,axis=1), model.predict(x_test)))

from sklearn.linear_model import LogisticRegression
model= LogisticRegression()
model.fit(x_train, np.argmax(y_train,axis=1))
model.score(x_test, np.argmax(y_test,axis=1))

from sklearn.metrics import classification_report
print(classification_report(np.argmax(y_test,axis=1), model.predict(x_test)))

from sklearn.tree import DecisionTreeClassifier
model= DecisionTreeClassifier()
model.fit(x_train, np.argmax(y_train,axis=1))
model.score(x_test, np.argmax(y_test,axis=1))

from sklearn.metrics import classification_report
print(classification_report(np.argmax(y_test,axis=1), model.predict(x_test)))

from sklearn.tree import DecisionTreeClassifier
model= DecisionTreeClassifier()
model.fit(x_train, np.argmax(y_train,axis=1))
model.score(x_test, np.argmax(y_test,axis=1))

"""## BATESIAN"""

def create_classifier():
    switch = Switch(num_experts, embed_dim, num_tokens_per_batch)
    transformer_block = TransformerBlock(ff_dim, num_heads, switch)

    inputs = layers.Input(shape=(num_tokens_per_example,))
    embedding_layer = TokenAndPositionEmbedding(
        num_tokens_per_example, vocab_size, embed_dim
    )
    x = embedding_layer(inputs)
    x = transformer_block(x)
    x = layers.GlobalAveragePooling1D()(x)
    x=keras.layers.Dropout(0.2)(x)
    x = layers.Dense(128, activation="relu")(x)
    x=keras.layers.Dropout(0.2)(x)
    x = layers.Dense(64, activation="relu")(x)
    outputs=tfp.layers.DenseVariational(units=1,
                  make_prior_fn=prior,
                  make_posterior_fn=posterior, activation='sigmoid',
                  kl_weight=1/x_train.shape[0])(x)

    classifier = keras.Model(inputs=inputs, outputs=outputs)
    return classifier

import keras.backend as K
def F1_Score(y_true, y_pred): #taken from old keras source code
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    recall = true_positives / (possible_positives + K.epsilon())
    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())
    return f1_val

def nll(y_true, y_pred):
  dist = tfp.distributions.Normal(loc=y_pred, scale=1.0)
  return tf.reduce_sum(-dist.log_prob(y_true))

def run_experiment(classifier):
    classifier.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.0001),
        loss=nll,
        metrics=[keras.metrics.CategoricalAccuracy(),keras.metrics.Recall(), keras.metrics.Precision(),F1_Score, tf.keras.metrics.AUC()],
    )
    history = classifier.fit(
        x_train,y_train/1.0,
        batch_size=50,
        epochs=20,
        validation_data=(x_test,y_test/1.0)
    )
    return history

classifier=create_classifier()
hist=run_experiment(classifier)

import matplotlib.pyplot as plt
plt.rcParams['figure.figsize']=(15,6)
pd.DataFrame(hist.history).plot()
plt.grid()
plt.gca().set_ylim(0,1)
plt.show()



